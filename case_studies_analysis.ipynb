{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01075f5e-3b01-4859-872c-19267c936132",
   "metadata": {},
   "source": [
    "## Case Studies Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82d3e6-d66f-45da-8a77-17256738d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import python package\n",
    "import os, sys\n",
    "sys.path.append('./scripts')\n",
    "import graph_tool.all as gt\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "import joblib\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from node_synonymizer import NodeSynonymizer\n",
    "synonymizer = NodeSynonymizer()\n",
    "import collections\n",
    "import itertools\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7513da-7015-406a-a9e0-a7b68ab153ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define some functions\n",
    "def get_name(graph_id):\n",
    "    if synonymizer.get_canonical_curies(graph_id)[graph_id]:\n",
    "        return synonymizer.get_canonical_curies(graph_id)[graph_id]['preferred_name']\n",
    "    else:\n",
    "        None\n",
    "\n",
    "def make_path(rel_ent_score):\n",
    "    rel_vec, ent_vec, score = rel_ent_score\n",
    "    return ['->'.join([id_to_name[id2entity[ent_vec[index]]]+'->'+id2relation[rel_vec[index+1]] for index in range(len(ent_vec)-1)] + [id_to_name[id2entity[ent_vec[len(ent_vec)-1]]]]), score]\n",
    "\n",
    "class knowledge_graph:\n",
    "\n",
    "    def __init__(self, data_dir, bandwidth=3000):\n",
    "        # Load data\n",
    "        self.bandwidth = bandwidth\n",
    "        self.entity2id, self.id2entity = utils.load_index(os.path.join(data_dir, 'entity2freq.txt'))\n",
    "        self.num_entities = len(self.entity2id)\n",
    "        self.relation2id, self.id2relation = utils.load_index(os.path.join(data_dir, 'relation2freq.txt'))\n",
    "\n",
    "        # Load graph structures\n",
    "        adj_list_path = os.path.join(data_dir, 'adj_list.pkl')\n",
    "        with open(adj_list_path, 'rb') as f:\n",
    "            self.adj_list = pickle.load(f)\n",
    "\n",
    "        self.page_rank_scores = self.load_page_rank_scores(os.path.join(data_dir, 'kg.pgrk'))\n",
    "\n",
    "        self.graph = {source:self.get_action_space(source) for source in range(self.num_entities)}\n",
    "\n",
    "    def load_page_rank_scores(self, input_path):\n",
    "        pgrk_scores = collections.defaultdict(float)\n",
    "        with open(input_path) as f:\n",
    "            for line in f:\n",
    "                entity, score = line.strip().split('\\t')\n",
    "                entity_id = self.entity2id[entity.strip()]\n",
    "                score = float(score)\n",
    "                pgrk_scores[entity_id] = score\n",
    "        return pgrk_scores\n",
    "\n",
    "\n",
    "    def get_action_space(self, source):\n",
    "        action_space = []\n",
    "        if source in self.adj_list:\n",
    "            for relation in self.adj_list[source]:\n",
    "                targets = self.adj_list[source][relation]\n",
    "                for target in targets:\n",
    "                    action_space.append((relation, target))\n",
    "            if len(action_space) + 1 >= self.bandwidth:\n",
    "                # Base graph pruning\n",
    "                sorted_action_space = sorted(action_space, key=lambda x: self.page_rank_scores[x[1]], reverse=True)\n",
    "                action_space = sorted_action_space[:self.bandwidth]\n",
    "        return action_space\n",
    "\n",
    "def check_curie(curie):\n",
    "    if curie is None:\n",
    "        return (None, None)\n",
    "    res = synonymizer.get_canonical_curies(curie)[curie]\n",
    "    if res is not None:\n",
    "        preferred_curie = synonymizer.get_canonical_curies(curie)[curie]['preferred_curie']\n",
    "    else:\n",
    "        preferred_curie = None\n",
    "    if preferred_curie in kg.entity2id:\n",
    "        return (preferred_curie, kg.entity2id[preferred_curie])\n",
    "    else:\n",
    "        return (preferred_curie, None)\n",
    "\n",
    "    \n",
    "def generate_graphml_files(pair):\n",
    "    edges = path_res_dict[pair]\n",
    "    nodes = set(list(edges[0]) + list(edges[1]))\n",
    "\n",
    "    g = gt.Graph(directed=True)\n",
    "    node_name = g.new_vertex_property(\"string\")\n",
    "    name_to_vertex = dict()\n",
    "    for index, name in enumerate(nodes):\n",
    "        v = g.add_vertex()\n",
    "        node_name[v] = name\n",
    "        name_to_vertex[name] = v\n",
    "\n",
    "    edge_relation = g.new_edge_property(\"string\")\n",
    "    for index in range(len(edges)):\n",
    "        source = edges.loc[index,0]\n",
    "        target = edges.loc[index,1]\n",
    "        e = g.add_edge(g.vertex(name_to_vertex[source]),g.vertex(name_to_vertex[target]))\n",
    "        edge_relation[e] = edges.loc[index,2].replace('biolink:','')\n",
    "\n",
    "    if not os.path.exists('graphml_files'):\n",
    "        os.makedirs('graphml_files')\n",
    "        \n",
    "    g.vertex_properties[\"node name\"] = node_name\n",
    "    g.edge_properties[\"edge relation\"] = edge_relation\n",
    "    g.save(os.path.join('graphml_files','_'.join([get_name(x) for x in pair]).replace(' ','_').lower()+'.graphml'), fmt='graphml')\n",
    "\n",
    "\n",
    "def my_draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=None,\n",
    "    label_pos=0.5,\n",
    "    font_size=10,\n",
    "    font_color=\"k\",\n",
    "    font_family=\"sans-serif\",\n",
    "    font_weight=\"normal\",\n",
    "    alpha=None,\n",
    "    bbox=None,\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    ax=None,\n",
    "    rotate=True,\n",
    "    clip_on=True,\n",
    "    rad=0\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    if edge_labels is None:\n",
    "        labels = {(u, v): d for u, v, d in G.edges(data=True)}\n",
    "    else:\n",
    "        labels = edge_labels\n",
    "    text_items = {}\n",
    "    for (n1, n2), label in labels.items():\n",
    "        (x1, y1) = pos[n1]\n",
    "        (x2, y2) = pos[n2]\n",
    "        (x, y) = (\n",
    "            x1 * label_pos + x2 * (1.0 - label_pos),\n",
    "            y1 * label_pos + y2 * (1.0 - label_pos),\n",
    "        )\n",
    "        pos_1 = ax.transData.transform(np.array(pos[n1]))\n",
    "        pos_2 = ax.transData.transform(np.array(pos[n2]))\n",
    "        linear_mid = 0.5*pos_1 + 0.5*pos_2\n",
    "        d_pos = pos_2 - pos_1\n",
    "        rotation_matrix = np.array([(0,1), (-1,0)])\n",
    "        ctrl_1 = linear_mid + rad*rotation_matrix@d_pos\n",
    "        ctrl_mid_1 = 0.5*pos_1 + 0.5*ctrl_1\n",
    "        ctrl_mid_2 = 0.5*pos_2 + 0.5*ctrl_1\n",
    "        bezier_mid = 0.5*ctrl_mid_1 + 0.5*ctrl_mid_2\n",
    "        (x, y) = ax.transData.inverted().transform(bezier_mid)\n",
    "\n",
    "        if rotate:\n",
    "            # in degrees\n",
    "            angle = np.arctan2(y2 - y1, x2 - x1) / (2.0 * np.pi) * 360\n",
    "            # make label orientation \"right-side-up\"\n",
    "            if angle > 90:\n",
    "                angle -= 180\n",
    "            if angle < -90:\n",
    "                angle += 180\n",
    "            # transform data coordinate angle to screen coordinate angle\n",
    "            xy = np.array((x, y))\n",
    "            trans_angle = ax.transData.transform_angles(\n",
    "                np.array((angle,)), xy.reshape((1, 2))\n",
    "            )[0]\n",
    "        else:\n",
    "            trans_angle = 0.0\n",
    "        # use default box of white with white border\n",
    "        if bbox is None:\n",
    "            bbox = dict(boxstyle=\"round\", ec=(1.0, 1.0, 1.0), fc=(1.0, 1.0, 1.0))\n",
    "        if not isinstance(label, str):\n",
    "            label = str(label)  # this makes \"1\" and 1 labeled the same\n",
    "\n",
    "        t = ax.text(\n",
    "            x,\n",
    "            y,\n",
    "            label,\n",
    "            size=font_size,\n",
    "            color=font_color,\n",
    "            family=font_family,\n",
    "            weight=font_weight,\n",
    "            alpha=alpha,\n",
    "            horizontalalignment=horizontalalignment,\n",
    "            verticalalignment=verticalalignment,\n",
    "            rotation=trans_angle,\n",
    "            transform=ax.transData,\n",
    "            bbox=bbox,\n",
    "            zorder=1,\n",
    "            clip_on=clip_on,\n",
    "        )\n",
    "        text_items[(n1, n2)] = t\n",
    "\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"both\",\n",
    "        bottom=False,\n",
    "        left=False,\n",
    "        labelbottom=False,\n",
    "        labelleft=False,\n",
    "    )\n",
    "\n",
    "    return text_items    \n",
    "\n",
    "def reorganize_df(edges):\n",
    "    temp = dict()\n",
    "    for index in range(len(edges)):\n",
    "        source, target = edges.loc[index,0],edges.loc[index,1]\n",
    "        if (source, target) not in temp:\n",
    "            temp[(source, target)] = dict()\n",
    "            temp[(source, target)]['edges'] = set([edges.loc[index,2].replace('biolink:','')])\n",
    "        else:\n",
    "            temp[(source, target)]['edges'].update(set([edges.loc[index,2].replace('biolink:','')]))\n",
    "\n",
    "    return pd.DataFrame([(key[0],key[1],'/'.join(list(value['edges']))) for key, value in temp.items()])\n",
    "\n",
    "def path_graph(pair, title, tp_score, is_in_train_set, is_in_not_train_set, rad=0.07):\n",
    "    edges = reorganize_df(path_res_dict[pair])\n",
    "    g = nx.DiGraph()\n",
    "    g.add_edges_from([(x[0],x[1], {'relation': x[2].replace('biolink:','')}) for x in edges.to_numpy()])\n",
    "    labels= nx.get_edge_attributes(g,'relation')\n",
    "\n",
    "    f = plt.figure(figsize=(18, 12));\n",
    "    ax = f.add_subplot(111);\n",
    "    pos = graphviz_layout(g, prog='dot', args='-Grankdir=\"LR\"');\n",
    "    nx.draw_networkx_nodes(g, pos, node_size=10, linewidths=0.5, alpha=0.9)\n",
    "    nx.draw_networkx_edges(g, pos, connectionstyle=f'arc3,rad={rad}', width=0.5)\n",
    "    nx.draw_networkx_labels(g, pos, font_size=12, font_color='blue');\n",
    "    my_draw_networkx_edge_labels(g, pos, edge_labels=labels, rotate=True, font_size=6, font_weight='bold', rad = rad);\n",
    "    plt.text(0.005, 0.06, f\"Predicted Score: {tp_score}\", size=10, color='black', fontweight='bold', transform=ax.transAxes);\n",
    "    plt.text(0.005, 0.03, f\"Is in train set: {is_in_train_set}\", size=10, color='black', fontweight='bold', transform=ax.transAxes);\n",
    "    plt.text(0.005, 0, f\"Is in val/test set: {is_in_not_train_set}\", size=10, color='black', fontweight='bold', transform=ax.transAxes);\n",
    "    plt.text(0.005, 0.97, f\"Drug ID in KG2c: {pair[0]} ({title.split(' - ')[0]})\", size=10, color='black', fontweight='bold', transform=ax.transAxes);\n",
    "    plt.text(0.005, 0.94, f\"Disease ID in KG2c: {pair[1]} ({title.split(' - ')[1]})\", size=10, color='black', fontweight='bold', transform=ax.transAxes);\n",
    "    return [f,g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584718f5-7832-48d0-9e47-e22fe854b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up some variables\n",
    "data_dir = './data'\n",
    "folder_name = 'RF_model_3class'\n",
    "model_name = 'RF_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547504a8-8098-48d1-bc7c-43d3dbe7f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read unsupervised GraphSage embedding vectors\n",
    "with open(os.path.join(data_dir, 'graphsage_output', 'unsuprvised_graphsage_entity_embeddings_1epoch_10000.pkl'), 'rb') as infile:\n",
    "    entity_embeddings_dict = pickle.load(infile)\n",
    "## generate mapping dictionaries\n",
    "entity2id, id2entity = utils.load_index(os.path.join(data_dir, 'entity2freq.txt'))\n",
    "relation2id, id2relation = utils.load_index(os.path.join(data_dir, 'relation2freq.txt'))\n",
    "all_graph_nodes_info = pd.read_csv(os.path.join(data_dir, 'all_graph_nodes_info.txt'), sep='\\t', header=0)\n",
    "all_graph_nodes_info['all_names'] = all_graph_nodes_info['all_names'].apply(eval)\n",
    "id_to_name = dict() \n",
    "for index in range(len(all_graph_nodes_info)):\n",
    "    if type(all_graph_nodes_info.loc[index,'name']) is str:\n",
    "        id_to_name[all_graph_nodes_info.loc[index,'id']] = all_graph_nodes_info.loc[index,'name']\n",
    "    else:\n",
    "        id_to_name[all_graph_nodes_info.loc[index,'id']] = all_graph_nodes_info.loc[index,'all_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7a221-037a-41bd-9f11-d4b67c486c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read purposed drug repurposing model\n",
    "fitModel = joblib.load(os.path.join('models', folder_name, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4ad70-ef68-4b40-88c1-747c3ec1574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up case studies disease ids\n",
    "## check more information from 'synonyms' service on https://arax.ncats.io/\n",
    "hemophilia_b_id = 'MONDO:0010604'\n",
    "huntington_disease_id = 'MONDO:0007739'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990f19e-6607-48f3-9d16-83414f874528",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all drugs ids\n",
    "type2id, id2type = utils.load_index(os.path.join(data_dir, 'type2freq.txt'))\n",
    "with open(os.path.join(data_dir, 'entity2typeid.pkl'), 'rb') as infile:\n",
    "    entity2typeid = pickle.load(infile)\n",
    "drug_type = ['biolink:Drug', 'biolink:SmallMolecule']\n",
    "drug_type_ids = [type2id[x] for x in drug_type]\n",
    "drug_ids = [id2entity[index] for index, typeid in enumerate(entity2typeid) if typeid in drug_type_ids]\n",
    "## read training, validation and test datasets\n",
    "train_rf_3class = pd.read_csv(os.path.join(data_dir, 'pretrain_reward_shaping_model_train_val_test_random_data_3class', 'train_pairs.txt'), sep='\\t', header=0)\n",
    "val_rf_3class = pd.read_csv(os.path.join(data_dir, 'pretrain_reward_shaping_model_train_val_test_random_data_3class', 'val_pairs.txt'), sep='\\t', header=0)\n",
    "test_rf_3class = pd.read_csv(os.path.join(data_dir, 'pretrain_reward_shaping_model_train_val_test_random_data_3class', 'test_pairs.txt'), sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652a645-1a7e-4f7d-8895-acb8e3c68520",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read customized RTX-KG2c into graph tools\n",
    "kg = knowledge_graph(data_dir, bandwidth=3000)\n",
    "G = gt.Graph()\n",
    "kg_tmp = dict()\n",
    "for source in kg.graph:\n",
    "    for (relation, target) in kg.graph[source]:\n",
    "        if (source, target) not in kg_tmp:\n",
    "            kg_tmp[(source, target)] = set([relation])\n",
    "        else:\n",
    "            kg_tmp[(source, target)].update(set([relation]))\n",
    "etype = G.new_edge_property('object')\n",
    "for (source, target) in kg_tmp:\n",
    "    e = G.add_edge(source,target)\n",
    "    etype[e] = kg_tmp[(source, target)]\n",
    "G.edge_properties['edge_type'] = etype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029e566-4b4c-4a75-a994-290432295155",
   "metadata": {},
   "source": [
    "### Case Studies Analysis with Hemophilia B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d90987-785d-436c-a77e-dcdc1358b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([np.hstack([entity_embeddings_dict[drug_id],entity_embeddings_dict[hemophilia_b_id]]) for drug_id in drug_ids])\n",
    "res_temp = fitModel.predict_proba(X)\n",
    "res = pd.concat([pd.DataFrame(drug_ids),pd.DataFrame([hemophilia_b_id]*len(drug_ids)),pd.DataFrame(res_temp)], axis=1)\n",
    "res.columns = ['drug_id','disease_id','tn_score','tp_score','unknown_score']\n",
    "res = res.sort_values(by=['tp_score'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6351ac4-dc95-4454-bf36-6fc46f397fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_in_train_set = pd.DataFrame(res['drug_id'].isin(list(train_rf_3class.loc[(train_rf_3class['target'].isin([hemophilia_b_id])) & (train_rf_3class['y']==1),'source'])))\n",
    "tp_not_in_train_set_list = list(val_rf_3class.loc[(val_rf_3class['target'].isin([hemophilia_b_id])) & (val_rf_3class['y']==1),'source']) + list(test_rf_3class.loc[(test_rf_3class['target'].isin([hemophilia_b_id])) & (test_rf_3class['y']==1),'source'])\n",
    "tp_not_in_train_set = pd.DataFrame(res['drug_id'].isin(tp_not_in_train_set_list))\n",
    "tn_in_train_set = pd.DataFrame(res['drug_id'].isin(list(train_rf_3class.loc[(train_rf_3class['target'].isin([hemophilia_b_id])) & (train_rf_3class['y']==0),'source'])))\n",
    "tn_not_in_train_set_list = list(val_rf_3class.loc[(val_rf_3class['target'].isin([hemophilia_b_id])) & (val_rf_3class['y']==0),'source']) + list(test_rf_3class.loc[(test_rf_3class['target'].isin([hemophilia_b_id])) & (test_rf_3class['y']==0),'source'])\n",
    "tn_not_in_train_set = pd.DataFrame(res['drug_id'].isin(tn_not_in_train_set_list))\n",
    "random_pairs_in_train_set = pd.DataFrame(res['drug_id'].isin(list(train_rf_3class.loc[(train_rf_3class['target'].isin([hemophilia_b_id])) & (train_rf_3class['y']==2),'source'])))\n",
    "random_pairs_not_in_train_set_list = list(val_rf_3class.loc[(val_rf_3class['target'].isin([hemophilia_b_id])) & (val_rf_3class['y']==2),'source']) + list(test_rf_3class.loc[(test_rf_3class['target'].isin([hemophilia_b_id])) & (test_rf_3class['y']==2),'source'])\n",
    "random_pairs_not_in_train_set = pd.DataFrame(res['drug_id'].isin(random_pairs_not_in_train_set_list))\n",
    "res = pd.concat([res,tp_in_train_set,tp_not_in_train_set,tn_in_train_set,tn_not_in_train_set,random_pairs_in_train_set,random_pairs_not_in_train_set], axis=1)\n",
    "res.columns = ['drug_id', 'disease_id', 'tn_score', 'tp_score', 'unknown_score', 'tp_in_train_set', 'tp_not_in_train_set', 'tn_in_train_set','tn_not_in_train_set','random_pairs_in_train_set','random_pairs_not_in_train_set']\n",
    "test = res.apply(lambda row: [get_name(row[0]),get_name(row[1])], axis=1, result_type='expand')\n",
    "res = pd.concat([res['drug_id'],test[0].str.lower(),res['disease_id'],test[1],res.loc[:,list(res.columns)[2:]]], axis=1).reset_index(drop=True)\n",
    "res.columns = ['drug_id', 'drug_name', 'disease_id', 'disease_name', 'tn_score', 'tp_score', 'unknown_score', 'tp_in_train_set', 'tp_not_in_train_set', 'tn_in_train_set','tn_not_in_train_set','random_pairs_in_train_set','random_pairs_not_in_train_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69fcea-e6e8-4b29-8ef5-1dccaf32c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show top 50 predicted results with top 5 in training set\n",
    "temp1 = pd.concat([res.loc[res['tp_in_train_set'],:][:5],res.loc[~res['tp_in_train_set'],:]]).reset_index(drop=True)\n",
    "hemophilia_b_top50 = temp1.loc[:50,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33bdd5-57ff-466d-ac6c-a0fc354462f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract all kg-based paths between the top 50 drug-disease pairs\n",
    "filtered_res_all_paths = dict()\n",
    "filter_edges = [kg.relation2id[edge] for edge in ['biolink:related_to','biolink:biolink:part_of','biolink:coexists_with','biolink:contraindicated_for']]\n",
    "for index1 in range(len(huntington_top50)):\n",
    "    print(index1)\n",
    "    source, target = huntington_top50.loc[index1,['drug_id','disease_id']]\n",
    "    all_paths = [list(path) for path in gt.all_paths(G, check_curie(source)[1], check_curie(target)[1], cutoff=3)]\n",
    "    entity_paths = []\n",
    "    relation_paths = []\n",
    "    for path in all_paths:\n",
    "        path_temp = []\n",
    "        for index2 in range(len(path)-1):\n",
    "            if index2 == 0:\n",
    "                path_temp += [path[index2], list(etype[G.edge(path[index2], path[index2+1])]), path[index2+1]]\n",
    "            else:\n",
    "                path_temp += [list(etype[G.edge(path[index2], path[index2+1])]), path[index2+1]]\n",
    "        flattened_paths = list(itertools.product(*map(lambda x: [x] if type(x) is not list else x, path_temp)))\n",
    "        for flattened_path in flattened_paths:\n",
    "            if len(flattened_path) == 7:\n",
    "                relation_paths += [[kg.relation2id['SELF_LOOP_RELATION']] + [x for index3, x in enumerate(flattened_path) if index3%2==1]]\n",
    "                entity_paths += [[x for index3, x in enumerate(flattened_path) if index3%2==0]]\n",
    "            elif len(flattened_path) == 5:\n",
    "                relation_paths += [[kg.relation2id['SELF_LOOP_RELATION']] + [x for index3, x in enumerate(flattened_path) if index3%2==1] + [kg.relation2id['SELF_LOOP_RELATION']]]\n",
    "                entity_paths += [[x for index3, x in enumerate(flattened_path) if index3%2==0] + [flattened_path[-1]]]\n",
    "            else:\n",
    "                logger.info(f\"Found weird path: {flattened_path}\")\n",
    "    edge_mat = torch.tensor(relation_paths)\n",
    "    node_mat = torch.tensor(np.array(entity_paths).astype(int))\n",
    "    temp = pd.DataFrame(edge_mat.numpy())\n",
    "    keep_index = list(temp.loc[~(temp[1].isin(filter_edges) | temp[2].isin(filter_edges) | temp[3].isin(filter_edges)),:].index)\n",
    "    filtered_res_all_paths[(source,target)] = [edge_mat[keep_index],node_mat[keep_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcef5a8-57a0-49e1-beae-be572e8475b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save paths and calcualte path probability based on the trained ADAC-based RL model\n",
    "if not os.path.exists(os.path.join('case_study_data','hemophilia_b')):\n",
    "    os.makedirs(os.path.join('case_study_data','hemophilia_b'))\n",
    "with open(os.path.join('case_study_data','hemophilia_b','paths.pkl'),'wb') as outfile:\n",
    "    pickle.dump(filtered_res_all_paths, outfile)\n",
    "## run the following python script (see main.sh)\n",
    "## python ${work_folder}/scripts/calculate_path_prob.py --log_dir ${work_folder}/log_folder --log_name case_study_Huntington.log --data_dir ${work_folder}/data --policy_net_file ${work_folder}/models/ADAC_model/policy_net/policy_model_epoch51.pt --target_paths_file ${work_folder}/case_study_data/hemophilia_b/paths.pkl --output_file ${work_folder}/case_study_data/huntington_disease/paths_prob.pkl --max_path 3 --bandwidth 3000 --bucket_interval 50 --pretrain_model_path ${work_folder}/models/RF_model_3class/RF_model.pt --use_gpu --state_history 2 --ac_hidden 512 512 --disc_hidden 512 512 --metadisc_hidden 512 256 --batch_size 1000 --factor 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205f6a0-f8af-4f35-b149-a4e8d9330537",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_res = dict()\n",
    "N = 10\n",
    "\n",
    "with open(os.path.join('case_study_data','hemophilia_b','paths_prob.pkl'),'rb') as infile:\n",
    "    paths_temp = pickle.load(infile)\n",
    "for (source, target) in paths_temp:\n",
    "    batch_paths = paths_temp[(source, target)]\n",
    "    if len(batch_paths[1]) == 0:\n",
    "        continue\n",
    "    temp = pd.DataFrame(batch_paths[0].numpy())\n",
    "    batch_paths = [batch_paths[0],batch_paths[1],batch_paths[2]]\n",
    "    pred_prob_scores = batch_paths[2]\n",
    "    sorted_scores, indices = torch.sort(pred_prob_scores, descending=True)\n",
    "    batch_paths_sorted = [batch_paths[0][indices], batch_paths[1][indices], sorted_scores]\n",
    "    # top_10_indexes = list(pd.DataFrame(batch_paths_sorted[1].numpy()).drop_duplicates().index[:10])\n",
    "    temp_dict = dict()\n",
    "    count = 0\n",
    "    top_10_indexes = []\n",
    "    for index, x in enumerate(batch_paths_sorted[1].numpy()):\n",
    "        if tuple(x) in temp_dict:\n",
    "            top_10_indexes += [index]\n",
    "        else:\n",
    "            count += 1\n",
    "            temp_dict[tuple(x)] = 1\n",
    "            top_10_indexes += [index]\n",
    "        if count == N:\n",
    "            break\n",
    "    res = [batch_paths_sorted[0][top_10_indexes], batch_paths_sorted[1][top_10_indexes], batch_paths_sorted[2][top_10_indexes]]\n",
    "    path_res[(source, target)] = [make_path([res[0][index].numpy(),res[1][index].numpy(), res[2][index].numpy().item()]) for index in range(len(res[0]))] + [fitModel.predict_proba(np.hstack([entity_embeddings_dict[source],entity_embeddings_dict[target]]).reshape(1,-1))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6472cf5-f134-40e4-a003-9a464ed4dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_res_dict = dict()\n",
    "for pair in path_res:\n",
    "    path_segment_set = set()\n",
    "    for path in path_res[pair][:-1]:\n",
    "        path_segment = path[0].split('->')\n",
    "        temp = set([(path_segment[index],path_segment[index+2],path_segment[index+1]) for index in range(0,len(path_segment)-2,2)])\n",
    "        path_segment_set.update(temp)\n",
    "    path_res_dict[pair] = pd.DataFrame(path_segment_set)\n",
    "\n",
    "for pair in path_res_dict:\n",
    "    if path_res_dict[pair].shape[0] == 0:\n",
    "        continue\n",
    "    drug_name = pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'drug_name'].to_numpy().item().capitalize()\n",
    "    disease_name = pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'disease_name'].to_numpy().item().capitalize()\n",
    "    title = f\"{drug_name} - {disease_name}\"\n",
    "    tp_score = round(pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'tp_score'].to_numpy().item(),6)\n",
    "    is_in_train_set = str(pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'tp_in_train_set'].to_numpy().item())\n",
    "    is_in_not_train_set = str(pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'tp_not_in_train_set'].to_numpy().item())\n",
    "    fig, g = path_graph(pair, title, tp_score, is_in_train_set, is_in_not_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1731a1c-da9b-495a-99e8-5a6670b4db04",
   "metadata": {},
   "source": [
    "### Case Studies Analysis with Huntington disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4388b8-3857-4df8-8caa-0b4ce8a8055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([np.hstack([entity_embeddings_dict[drug_id],entity_embeddings_dict[huntington_disease_id]]) for drug_id in drug_ids])\n",
    "res_temp = fitModel.predict_proba(X)\n",
    "res = pd.concat([pd.DataFrame(drug_ids),pd.DataFrame([huntington_disease_id]*len(drug_ids)),pd.DataFrame(res_temp)], axis=1)\n",
    "res.columns = ['drug_id','disease_id','tn_score','tp_score','unknown_score']\n",
    "res = res.sort_values(by=['tp_score'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428c033-608e-4414-a7ac-95fa98bb7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_in_train_set = pd.DataFrame(res['drug_id'].isin(list(train_rf_3class.loc[(train_rf_3class['target'].isin([huntington_disease_id])) & (train_rf_3class['y']==1),'source'])))\n",
    "tp_not_in_train_set_list = list(val_rf_3class.loc[(val_rf_3class['target'].isin([huntington_disease_id])) & (val_rf_3class['y']==1),'source']) + list(test_rf_3class.loc[(test_rf_3class['target'].isin([huntington_disease_id])) & (test_rf_3class['y']==1),'source'])\n",
    "tp_not_in_train_set = pd.DataFrame(res['drug_id'].isin(tp_not_in_train_set_list))\n",
    "tn_in_train_set = pd.DataFrame(res['drug_id'].isin(list(train_rf_3class.loc[(train_rf_3class['target'].isin([huntington_disease_id])) & (train_rf_3class['y']==0),'source'])))\n",
    "tn_not_in_train_set_list = list(val_rf_3class.loc[(val_rf_3class['target'].isin([huntington_disease_id])) & (val_rf_3class['y']==0),'source']) + list(test_rf_3class.loc[(test_rf_3class['target'].isin([huntington_disease_id])) & (test_rf_3class['y']==0),'source'])\n",
    "tn_not_in_train_set = pd.DataFrame(res['drug_id'].isin(tn_not_in_train_set_list))\n",
    "random_pairs_in_train_set = pd.DataFrame(res['drug_id'].isin(list(train_rf_3class.loc[(train_rf_3class['target'].isin([huntington_disease_id])) & (train_rf_3class['y']==2),'source'])))\n",
    "random_pairs_not_in_train_set_list = list(val_rf_3class.loc[(val_rf_3class['target'].isin([huntington_disease_id])) & (val_rf_3class['y']==2),'source']) + list(test_rf_3class.loc[(test_rf_3class['target'].isin([huntington_disease_id])) & (test_rf_3class['y']==2),'source'])\n",
    "random_pairs_not_in_train_set = pd.DataFrame(res['drug_id'].isin(random_pairs_not_in_train_set_list))\n",
    "res = pd.concat([res,tp_in_train_set,tp_not_in_train_set,tn_in_train_set,tn_not_in_train_set,random_pairs_in_train_set,random_pairs_not_in_train_set], axis=1)\n",
    "res.columns = ['drug_id', 'disease_id', 'tn_score', 'tp_score', 'unknown_score', 'tp_in_train_set', 'tp_not_in_train_set', 'tn_in_train_set','tn_not_in_train_set','random_pairs_in_train_set','random_pairs_not_in_train_set']\n",
    "test = res.apply(lambda row: [get_name(row[0]),get_name(row[1])], axis=1, result_type='expand')\n",
    "res = pd.concat([res['drug_id'],test[0].str.lower(),res['disease_id'],test[1],res.loc[:,list(res.columns)[2:]]], axis=1).reset_index(drop=True)\n",
    "res.columns = ['drug_id', 'drug_name', 'disease_id', 'disease_name', 'tn_score', 'tp_score', 'unknown_score', 'tp_in_train_set', 'tp_not_in_train_set', 'tn_in_train_set','tn_not_in_train_set','random_pairs_in_train_set','random_pairs_not_in_train_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d249f31-6619-4b00-acad-db4885533750",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show top 50 predicted results with top 5 in training set\n",
    "temp1 = pd.concat([res.loc[res['tp_in_train_set'],:][:5],res.loc[~res['tp_in_train_set'],:]]).reset_index(drop=True)\n",
    "huntington_top50 = temp1.loc[:50,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31734ac-e9c4-4af9-ae70-95be3e5bac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract all kg-based paths between the top 50 drug-disease pairs\n",
    "filtered_res_all_paths = dict()\n",
    "filter_edges = [kg.relation2id[edge] for edge in ['biolink:related_to','biolink:biolink:part_of','biolink:coexists_with','biolink:contraindicated_for']]\n",
    "for index1 in range(len(huntington_top50)):\n",
    "    print(index1)\n",
    "    source, target = huntington_top50.loc[index1,['drug_id','disease_id']]\n",
    "    all_paths = [list(path) for path in gt.all_paths(G, check_curie(source)[1], check_curie(target)[1], cutoff=3)]\n",
    "    entity_paths = []\n",
    "    relation_paths = []\n",
    "    for path in all_paths:\n",
    "        path_temp = []\n",
    "        for index2 in range(len(path)-1):\n",
    "            if index2 == 0:\n",
    "                path_temp += [path[index2], list(etype[G.edge(path[index2], path[index2+1])]), path[index2+1]]\n",
    "            else:\n",
    "                path_temp += [list(etype[G.edge(path[index2], path[index2+1])]), path[index2+1]]\n",
    "        flattened_paths = list(itertools.product(*map(lambda x: [x] if type(x) is not list else x, path_temp)))\n",
    "        for flattened_path in flattened_paths:\n",
    "            if len(flattened_path) == 7:\n",
    "                relation_paths += [[kg.relation2id['SELF_LOOP_RELATION']] + [x for index3, x in enumerate(flattened_path) if index3%2==1]]\n",
    "                entity_paths += [[x for index3, x in enumerate(flattened_path) if index3%2==0]]\n",
    "            elif len(flattened_path) == 5:\n",
    "                relation_paths += [[kg.relation2id['SELF_LOOP_RELATION']] + [x for index3, x in enumerate(flattened_path) if index3%2==1] + [kg.relation2id['SELF_LOOP_RELATION']]]\n",
    "                entity_paths += [[x for index3, x in enumerate(flattened_path) if index3%2==0] + [flattened_path[-1]]]\n",
    "            else:\n",
    "                logger.info(f\"Found weird path: {flattened_path}\")\n",
    "    edge_mat = torch.tensor(relation_paths)\n",
    "    node_mat = torch.tensor(np.array(entity_paths).astype(int))\n",
    "    temp = pd.DataFrame(edge_mat.numpy())\n",
    "    keep_index = list(temp.loc[~(temp[1].isin(filter_edges) | temp[2].isin(filter_edges) | temp[3].isin(filter_edges)),:].index)\n",
    "    filtered_res_all_paths[(source,target)] = [edge_mat[keep_index],node_mat[keep_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f8384-a454-4537-9b4e-76ec192270b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save paths and calcualte path probability based on the trained ADAC-based RL model\n",
    "if not os.path.exists(os.path.join('case_study_data','huntington_disease')):\n",
    "    os.makedirs(os.path.join('case_study_data','huntington_disease'))\n",
    "with open(os.path.join('case_study_data','huntington_disease','paths.pkl'),'wb') as outfile:\n",
    "    pickle.dump(filtered_res_all_paths, outfile)\n",
    "## run the following python script (see main.sh)\n",
    "## python ${work_folder}/scripts/calculate_path_prob.py --log_dir ${work_folder}/log_folder --log_name case_study_Huntington.log --data_dir ${work_folder}/data --policy_net_file ${work_folder}/models/ADAC_model/policy_net/policy_model_epoch51.pt --target_paths_file ${work_folder}/case_study_data/huntington_disease/paths.pkl --output_file ${work_folder}/case_study_data/huntington_disease/paths_prob.pkl --max_path 3 --bandwidth 3000 --bucket_interval 50 --pretrain_model_path ${work_folder}/models/RF_model_3class/RF_model.pt --use_gpu --state_history 2 --ac_hidden 512 512 --disc_hidden 512 512 --metadisc_hidden 512 256 --batch_size 1000 --factor 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7103c49-92ea-4592-b733-008b8a1b924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_res = dict()\n",
    "N = 10\n",
    "\n",
    "with open(os.path.join('case_study_data','huntington_disease','paths_prob.pkl'),'rb') as infile:\n",
    "    paths_temp = pickle.load(infile)\n",
    "for (source, target) in paths_temp:\n",
    "    batch_paths = paths_temp[(source, target)]\n",
    "    if len(batch_paths[1]) == 0:\n",
    "        continue\n",
    "    temp = pd.DataFrame(batch_paths[0].numpy())\n",
    "    batch_paths = [batch_paths[0],batch_paths[1],batch_paths[2]]\n",
    "    pred_prob_scores = batch_paths[2]\n",
    "    sorted_scores, indices = torch.sort(pred_prob_scores, descending=True)\n",
    "    batch_paths_sorted = [batch_paths[0][indices], batch_paths[1][indices], sorted_scores]\n",
    "    # top_10_indexes = list(pd.DataFrame(batch_paths_sorted[1].numpy()).drop_duplicates().index[:10])\n",
    "    temp_dict = dict()\n",
    "    count = 0\n",
    "    top_10_indexes = []\n",
    "    for index, x in enumerate(batch_paths_sorted[1].numpy()):\n",
    "        if tuple(x) in temp_dict:\n",
    "            top_10_indexes += [index]\n",
    "        else:\n",
    "            count += 1\n",
    "            temp_dict[tuple(x)] = 1\n",
    "            top_10_indexes += [index]\n",
    "        if count == N:\n",
    "            break\n",
    "    res = [batch_paths_sorted[0][top_10_indexes], batch_paths_sorted[1][top_10_indexes], batch_paths_sorted[2][top_10_indexes]]\n",
    "    path_res[(source, target)] = [make_path([res[0][index].numpy(),res[1][index].numpy(), res[2][index].numpy().item()]) for index in range(len(res[0]))] + [fitModel.predict_proba(np.hstack([entity_embeddings_dict[source],entity_embeddings_dict[target]]).reshape(1,-1))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009a0b2-8e2f-4bb9-a786-4853dd40316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_res_dict = dict()\n",
    "for pair in path_res:\n",
    "    path_segment_set = set()\n",
    "    for path in path_res[pair][:-1]:\n",
    "        path_segment = path[0].split('->')\n",
    "        temp = set([(path_segment[index],path_segment[index+2],path_segment[index+1]) for index in range(0,len(path_segment)-2,2)])\n",
    "        path_segment_set.update(temp)\n",
    "    path_res_dict[pair] = pd.DataFrame(path_segment_set)\n",
    "\n",
    "for pair in path_res_dict:\n",
    "    if path_res_dict[pair].shape[0] == 0:\n",
    "        continue\n",
    "    drug_name = pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'drug_name'].to_numpy().item().capitalize()\n",
    "    disease_name = pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'disease_name'].to_numpy().item().capitalize()\n",
    "    title = f\"{drug_name} - {disease_name}\"\n",
    "    tp_score = round(pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'tp_score'].to_numpy().item(),6)\n",
    "    is_in_train_set = str(pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'tp_in_train_set'].to_numpy().item())\n",
    "    is_in_not_train_set = str(pred_res.loc[(pred_res['drug_id']==pair[0]) & (pred_res['disease_id']==pair[1]),'tp_not_in_train_set'].to_numpy().item())\n",
    "    fig, g = path_graph(pair, title, tp_score, is_in_train_set, is_in_not_train_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.12 (mypersonal_env)",
   "language": "python",
   "name": "mypersonal_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12 | packaged by conda-forge | (default, Jan 30 2022, 23:53:36) \n[GCC 9.4.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
